{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92588c75",
   "metadata": {},
   "source": [
    "## Modelos y Simulación – Trabajo Práctico \n",
    "\n",
    "## Modelización de Lenguaje Natural con CFG y SDT (Clase – Modelos y Simulación)\n",
    "\n",
    "**Autor: Cátedra – Modelos y Simulación - Facultad de Ingeniería - UCA**     \n",
    "**Fecha: 2025-15-09**   \n",
    "**Documento: MyS_C4A_Modelado Formal con Gramaticas _R2**   \n",
    "### Temas: **Análisis Léxico**, **Análisis Sintáctico (CFG)**, **Análisis Semántico** y **Traducción Dirigida por la Sintaxis (SDT)**.  \n",
    "####  Contexto: \n",
    "- Ejercitar el proceso del procesamiento de lenguaje: *tokens → parse (CFG) → semántica (SDT)*.\n",
    "- Implementar un **compositor semántico** paso a paso (λ-cálculo simple) para una oración transitiva.\n",
    "- Validar **tipos semánticos** y restricciones de selección.\n",
    "- Construir un **chatbot** de dominio acotado con **CFG + SDT**.\n",
    "- Explorar un **parser NLTK** sobre una gramática mínima.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dad111b-68f8-4875-8c95-164314bff6d3",
   "metadata": {},
   "source": [
    "Requisitos: \n",
    "%pip install ply\n",
    "%pip install NLTK\n",
    "%pip install ipwidgets\n",
    "%pip install unicode\n",
    "%pip install unidecode\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f030eec",
   "metadata": {},
   "source": [
    "```\n",
    "texto\n",
    "  └─► Análisis Léxico (tokenización, normalización)\n",
    "        └─► Análisis Sintáctico (CFG) ─► Árbol / derivación\n",
    "              └─► Análisis Semántico (SDT, λ-cálculo) ─► Fórmula lógica / AST\n",
    "                    └─► Motor de Respuestas (reglas, plantillas, acciones)\n",
    "                          └─► Salida (texto, acción)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b71e73",
   "metadata": {},
   "source": [
    "### Ejemplo de chatbot simple con reglas\n",
    "#### Lexer con Flex — chatbot.l\n",
    "#### Parser con Bison - chatbot.y\n",
    "- Convierte texto a tokens.\n",
    "- Normaliza: ignoramos mayúsculas con caseless. (Acentos: asumimos que ya vienen sin acentos como parte del preprocesamiento.)\n",
    "- Cómo ejecutar desde linux:\n",
    "  - flex -o lexer.c chatbot.l\n",
    "  - bison -d -o parser.c chatbot.y\n",
    "  - gcc -o chatbot parser.c lexer.c -lfl\n",
    "  - ./chatbot\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35bad7a4-d004-40e8-9c95-a92019026dab",
   "metadata": {},
   "source": [
    "/* chatbot.l */\n",
    "%option noyywrap\n",
    "%option yylineno\n",
    "%option caseless\n",
    "\n",
    "%{\n",
    "  #include \"parser.h\"   /* Header que genera Bison con -d: declara tokens */\n",
    "  #include <string.h>\n",
    "%}\n",
    "\n",
    "/* Reglas de palabras clave del dominio */\n",
    "HOLA           (\"hola\"|\"hol\")\n",
    "HOL\t\t\t   \"hol\"\n",
    "BUENOS_DIAS    \"buenos_dias\"\n",
    "BUENAS_NOCHES  \"buenas_noches\"\n",
    "QUE            \"que\"\n",
    "HORA           \"hora\"\n",
    "ES             \"es\"\n",
    "QUIERO         \"quiero\"\n",
    "INFORMACION    \"informacion\"\n",
    "SOBRE          \"sobre\"\n",
    "CURSOS         \"cursos\"\n",
    "\n",
    "/* Tokens auxiliares */\n",
    "EOL            \\n\n",
    "WS             [ \\t\\r]+\n",
    "WORD           [[:alpha:]_]+\n",
    "OTHER          .\n",
    "\n",
    "%%\n",
    "\n",
    "{HOLA}           { return HOLA; }\n",
    "{BUENOS_DIAS}    { return BUENOS_DIAS; }\n",
    "{BUENAS_NOCHES}  { return BUENAS_NOCHES; }\n",
    "\n",
    "{QUE}            { return QUE; }\n",
    "{HORA}           { return HORA; }\n",
    "{ES}             { return ES; }\n",
    "\n",
    "{QUIERO}         { return QUIERO; }\n",
    "{INFORMACION}    { return INFORMACION; }\n",
    "{SOBRE}          { return SOBRE; }\n",
    "{CURSOS}         { return CURSOS; }\n",
    "\n",
    "{EOL}            { return EOL; }\n",
    "{WS}             { /* ignorar */ }\n",
    "\n",
    "\n",
    "{WORD}           { return OTHER; }\n",
    "{OTHER}          { return OTHER; }\n",
    "\n",
    "%%\n",
    "\n",
    "/* sin main aquí; Bison define yyparse() y lo invocaremos allí */\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dcce5d-9a09-4831-940f-68c73cd9162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "/* chatbot.y */\n",
    "%{\n",
    "  #include <stdio.h>\n",
    "  #include <stdlib.h>\n",
    "\n",
    "  /* Prototipos provistos por Flex */\n",
    "  int yylex(void);\n",
    "  void yyerror(const char *s);\n",
    "\n",
    "%}\n",
    "\n",
    "/* Pedimos a Bison que genere el header con tokens (parser.h) usando -d */\n",
    "%define api.value.type {int}\n",
    "\n",
    "/* Declaración de tokens (deben coincidir con los que retorna Flex) */\n",
    "%token HOLA BUENOS_DIAS HOL BUENAS_NOCHES\n",
    "%token QUE HORA ES\n",
    "%token QUIERO INFORMACION SOBRE CURSOS\n",
    "%token EOL OTHER\n",
    "\n",
    "/* Precedencias no necesarias aquí, pero podrían usarse en gramáticas más complejas */\n",
    "\n",
    "%%\n",
    "\n",
    "/*  Gramática de alto nivel  */\n",
    "\n",
    "/* Permite múltiples líneas (comandos) */\n",
    "input\n",
    "  : /* vacío */\n",
    "  | input linea\n",
    "  ;\n",
    "\n",
    "linea\n",
    "  : SALUDO EOL      { puts(\"Chatbot: ¡Hola! ¿Cómo estás?\"); }\n",
    "  | PREGUNTA EOL    { puts(\"Chatbot: No tengo reloj, pero puedo ayudarte con cursos :)\"); }\n",
    "  | SOLICITUD EOL   { puts(\"Chatbot: Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\"); }\n",
    "  | basura EOL      { puts(\"Chatbot: Disculpa, no entendí. ¿Podés reformular?\"); }\n",
    "  | EOL             { /* línea vacía: ignorar */ }\n",
    "  ;\n",
    "\n",
    "SALUDO\n",
    "  : HOLA\n",
    "  | BUENOS_DIAS\n",
    "  | BUENAS_NOCHES\n",
    "  ;\n",
    "\n",
    "PREGUNTA\n",
    "  : QUE HORA ES\n",
    "  ;\n",
    "\n",
    "SOLICITUD\n",
    "  : QUIERO INFORMACION SOBRE CURSOS\n",
    "  ;\n",
    "\n",
    "/* Si la línea contiene tokens desconocidos o estructura no reconocida */\n",
    "basura\n",
    "  : basura OTHER\n",
    "  | OTHER\n",
    "  ;\n",
    "\n",
    "%%\n",
    "\n",
    "/*  Código en C embebido: main, yyerror  */\n",
    "\n",
    "int main(void) {\n",
    "  /* yyparse consume tokens de yylex() (Flex) y dispara acciones */\n",
    "  if (yyparse() == 0) {\n",
    "    /* parseo exitoso */\n",
    "    return EXIT_SUCCESS;\n",
    "  } else {\n",
    "    return EXIT_FAILURE;\n",
    "  }\n",
    "}\n",
    "\n",
    "void yyerror(const char *s) {\n",
    "  /* Mensajes de error sintáctico */\n",
    "  fprintf(stderr, \"Parser error: %s\\n\", s);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e74633-8637-4c29-a779-c97884a97726",
   "metadata": {},
   "source": [
    "- En el contexto de chatbots, NLP e IA conversacional, un intent (intención) es la meta o propósito del usuario al escribir un mensaje. Es lo que quiere lograr la persona, más allá de las palabras exactas que usa.\n",
    "- El uuario escribe:\n",
    "  - \"hola\", \"buen día\" → Intent: SALUDO\n",
    "  - \"qué hora es\", \"me decís la hora?\" → Intent: PREGUNTA_HORA\n",
    "  - \"quiero información sobre cursos\", \"necesito capacitaciones\" → Intent: SOLICITUD_INFO\n",
    "- El sistema mapea frases distintas al mismo intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d8c466-b103-4b7f-a2e5-2ad17c1ef408",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chatbot_ply.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbot_ply.py\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Chatbot con PLY (Python Lex-Yacc)\n",
    "    --\n",
    "Intents soportados:\n",
    "- SALUDO: \"hola\", \"buenas\", \"buenos dias\"\n",
    "- PREGUNTA: \"que hora es\", \"me decis la hora\", \"tenes la hora\", etc.\n",
    "- SOLICITUD: \"quiero informacion sobre cursos [sobre TOPIC]\" y variantes\n",
    "\n",
    "Ejecutar con:\n",
    "  pip install ply\n",
    "  python chatbot_ply.py\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "#    Dependencia   \n",
    "try:\n",
    "    import ply.lex as lex\n",
    "    import ply.yacc as yacc\n",
    "except Exception:\n",
    "    sys.stderr.write(\"Necesitás instalar PLY primero: pip install ply\\n\")\n",
    "    raise\n",
    "\n",
    " \n",
    "# 1) Lexer (tokens, sinónimos, slot)\n",
    " \n",
    "\n",
    "# Lista de tokens que usará el parser\n",
    "tokens = (\n",
    "    'HOLA',\n",
    "    'QUEHORA',\n",
    "    'QUIERO',\n",
    "    'INFORMACION',\n",
    "    'SOBRE',\n",
    "    'CURSOS',\n",
    "    'TOPIC',\n",
    "    'EOL',\n",
    ")\n",
    "\n",
    "# Ignorar espacios y tabs\n",
    "t_ignore = ' \\t\\r'\n",
    "\n",
    "# Ignorar puntuación (ruido)\n",
    "def t_PUNCT(t):\n",
    "    r'[\\,\\.\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}\\-]+'\n",
    "    pass\n",
    "\n",
    "# Fin de línea -> EOL\n",
    "def t_EOL(t):\n",
    "    r'\\n+'\n",
    "    return t\n",
    "\n",
    "# Frases para “qué hora es” → un único token (tolerante a variantes)\n",
    "def t_QUEHORA(t):\n",
    "    r'(que\\s+hora\\s+es)|(me\\s+dec[ií]s\\s+la\\s+hora)|(ten[eé]s\\s+la\\s+hora)|(tienes\\s+la\\s+hora)|(^|\\s)hora(\\s|$)'\n",
    "    t.value = 'que_hora_es'\n",
    "    return t\n",
    "\n",
    "# Saludos\n",
    "def t_HOLA(t):\n",
    "    r'(hola)|(buenas(\\s+(tardes|noches))?)|(buenos\\s+d[ií]as)'\n",
    "    t.value = 'hola'\n",
    "    return t\n",
    "\n",
    "# Verbos de intención\n",
    "def t_QUIERO(t):\n",
    "    r'(quiero)|(quisiera)|(me\\s+gustar[ií]a)'\n",
    "    t.value = 'quiero'\n",
    "    return t\n",
    "\n",
    "# Info / data\n",
    "def t_INFORMACION(t):\n",
    "    r'(informaci[oó]n)|(info)|(data)|(detalles)'\n",
    "    t.value = 'informacion'\n",
    "    return t\n",
    "\n",
    "# Preposición/conector\n",
    "def t_SOBRE(t):\n",
    "    r'(sobre)|(acerca\\s+de)|(de)'\n",
    "    t.value = 'sobre'\n",
    "    return t\n",
    "\n",
    "# Cursos y sinónimos\n",
    "def t_CURSOS(t):\n",
    "    r'(curso|cursos|capacitaci[oó]n|capacitaciones|formaci[oó]n)'\n",
    "    t.value = 'cursos'\n",
    "    return t\n",
    "\n",
    "# TOPIC libre (palabras con espacios luego serán tomadas por la gramática)\n",
    "def t_TOPIC(t):\n",
    "    r'[A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9][A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9\\ \\-\\_]*'\n",
    "    # Nota: el parser lo usa solo después de \"SOBRE\"\n",
    "    t.value = t.value.strip()\n",
    "    return t\n",
    "\n",
    "# Errores léxicos: avanzar sin trabar\n",
    "def t_error(t):\n",
    "    t.lexer.skip(1)\n",
    "\n",
    "# Compilar lexer (case-insensitive)\n",
    "lexer = lex.lex(reflags=re.IGNORECASE | re.UNICODE)\n",
    "\n",
    " \n",
    "# 2) Parser (gramática PLY)\n",
    " \n",
    "\n",
    "# No necesitamos precedencia aquí\n",
    "precedence = ()\n",
    "\n",
    "# Entrada: una o más líneas\n",
    "def p_input(p):\n",
    "    '''input : input linea\n",
    "             | linea'''\n",
    "    pass\n",
    "\n",
    "def p_linea(p):\n",
    "    '''linea : saludo EOL\n",
    "             | pregunta_hora EOL\n",
    "             | solicitud_cursos EOL\n",
    "             | EOL'''\n",
    "    # Acciones implementadas en subreglas\n",
    "\n",
    "def p_saludo(p):\n",
    "    'saludo : HOLA'\n",
    "    print(\"Chatbot: ¡Hola! ¿Cómo estás?\")\n",
    "\n",
    "def p_pregunta_hora(p):\n",
    "    'pregunta_hora : QUEHORA'\n",
    "    print(\"Chatbot: No tengo reloj, pero puedo ayudarte con cursos :)\")\n",
    "\n",
    "# Variantes de solicitud:\n",
    "# 1) QUIERO (INFORMACION)? (SOBRE)? CURSOS (SOBRE TOPIC)?\n",
    "# 2) QUIERO CURSOS (SOBRE TOPIC)?\n",
    "# 3) QUIERO INFORMACION (SOBRE TOPIC)?\n",
    "def p_solicitud_cursos(p):\n",
    "    '''solicitud_cursos : QUIERO opt_info opt_sobre CURSOS opt_tema\n",
    "                        | QUIERO CURSOS opt_tema\n",
    "                        | QUIERO INFORMACION opt_tema'''\n",
    "    if len(p) == 6:\n",
    "        topic = p[5]\n",
    "    elif len(p) == 4:\n",
    "        topic = p[3]\n",
    "    else:\n",
    "        topic = None\n",
    "\n",
    "    if topic:\n",
    "        print(f'Chatbot: Tenemos cursos de ML, Simulación y Redes sobre \"{topic}\". ¿Cuál te interesa?')\n",
    "    else:\n",
    "        print('Chatbot: Tenemos cursos de ML, Simulación y Redes. ¿Cuál te interesa?')\n",
    "\n",
    "def p_opt_info(p):\n",
    "    '''opt_info : INFORMACION\n",
    "                | empty'''\n",
    "    pass\n",
    "\n",
    "def p_opt_sobre(p):\n",
    "    '''opt_sobre : SOBRE\n",
    "                 | empty'''\n",
    "    pass\n",
    "\n",
    "def p_opt_tema(p):\n",
    "    '''opt_tema : empty\n",
    "                | SOBRE tema'''\n",
    "    if len(p) == 3:\n",
    "        p[0] = p[2]\n",
    "    else:\n",
    "        p[0] = None\n",
    "\n",
    "def p_tema(p):\n",
    "    'tema : TOPIC'\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_empty(p):\n",
    "    'empty :'\n",
    "    pass\n",
    "\n",
    "# Recuperación de errores: consumir hasta EOL\n",
    "def p_error(p):\n",
    "    if p is None:\n",
    "        return\n",
    "    tok = p\n",
    "    while tok and tok.type != 'EOL':\n",
    "        tok = parser.token()\n",
    "    print(\"Chatbot: Disculpa, no entendí. ¿Podés reformular?\")\n",
    "\n",
    "# Construir parser\n",
    "parser = yacc.yacc(start='input', debug=False, write_tables=False)\n",
    "\n",
    " \n",
    "# 3) REPL (bucle interactivo)\n",
    " \n",
    "BANNER = (\n",
    "    \"Chatbot PLY — escribí una línea y Enter (Ctrl+D/Ctrl+Z para salir)\\n\"\n",
    "    \"Ejemplos: 'hola', 'que hora es', 'quiero informacion sobre cursos', \"\n",
    "    \"'quisiera cursos sobre deep learning'\\n\"\n",
    ")\n",
    "\n",
    "def main():\n",
    "    print(BANNER)\n",
    "    buffer = \"\"\n",
    "    try:\n",
    "        for line in sys.stdin:\n",
    "            buffer += line\n",
    "            if not buffer.endswith(\"\\n\"):\n",
    "                continue\n",
    "            parser.parse(buffer, lexer=lexer)\n",
    "            buffer = \"\"\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3445e49",
   "metadata": {},
   "source": [
    "## 2. Gramática Libre de Contexto (CFG)\n",
    "\n",
    "Usaremos una **CFG mínima** para oraciones transitivas con objeto introducido por preposición **a**:\n",
    "\n",
    "Producciones:\n",
    "```\n",
    "S   → NP VP\n",
    "VP  → V NP_a\n",
    "NP  → Det N\n",
    "NP_a→ 'a' NP\n",
    "Det → 'el'\n",
    "N   → 'gato' | 'perro'\n",
    "V   → 'muerde'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b6f92",
   "metadata": {},
   "source": [
    "## 3. SDT (Traducción Dirigida por la Sintaxis) con λ-cálculo simple\n",
    "\n",
    "Asociamos a cada símbolo un atributo semántico `.sem`. Léxico y reglas:\n",
    "\n",
    "**Léxico (semántica):**\n",
    "- `N.gato.sem   gato1`\n",
    "- `N.perro.sem   perro1`\n",
    "- `Det.el.sem   λx.x`  (definido)\n",
    "- `V.muerde.sem   λobj.λsubj. MORDER(subj, obj)`\n",
    "\n",
    "**Reglas (composición):**\n",
    "- `NP  → Det N`        ⇒  `NP.sem   Det.sem(N.sem)`\n",
    "- `NP_a→ 'a' NP`       ⇒  `NP_a.sem   NP.sem`  (la preposición no cambia la entidad en esta demo)\n",
    "- `VP  → V NP_a`       ⇒  `VP.sem   λsubj. V.sem(NP_a.sem)(subj)`\n",
    "  - El resultado es una función que, dado un sujeto, devuelve una proposición. “aplico el verbo al objeto para obtener un VP que es un predicado de sujetos”\n",
    "- `S   → NP VP`        ⇒  `S.sem   VP.sem(NP.sem)`\n",
    "\n",
    "Ejemplo objetivo: **\"el gato muerde al perro\"** → `MORDER(gato1, perro1)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62f89e",
   "metadata": {},
   "source": [
    "## 4. Implementación del compositor semántico.\n",
    "\n",
    "- Implementamos las acciones semánticas como funciones Python y un *parser manual* que asume la estructura esperada.\n",
    "- Chatbot basado en CFG + SDT (dominio acotado)¶\n",
    "- Usamos una clasificación por patrones equivalente a una gramática mínima y asociamos acciones semánticas (respuestas) a cada intención.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "422dd412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chatbotCFG_ply.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile chatbotCFG_ply.py\n",
    "# -*- coding: utf-8 -*-\n",
    "# PLY chatbot CFG \n",
    "\n",
    "import re, sys\n",
    "import ply.lex as lex\n",
    "import ply.yacc as yacc\n",
    "import ply.lex as _lexmod\n",
    "\n",
    "#   LEXER (clase)  \n",
    "class ChatLexer:\n",
    "    tokens = (\n",
    "        'HOLA','QUEHORA','QUIERO','INFORMACION','SOBRE','CURSOS','TOPIC','EOL'\n",
    "    )\n",
    "    t_ignore = ' \\t\\r'\n",
    "\n",
    "    def t_PUNCT(self, t):\n",
    "        r'[\\,\\.\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}\\-]+'\n",
    "        pass\n",
    "\n",
    "    def t_EOL(self, t):\n",
    "        r'\\n+'\n",
    "        return t\n",
    "\n",
    "    def t_QUEHORA(self, t):\n",
    "        r'(que\\s+hora\\s+es)|(me\\s+dec[ií]s\\s+la\\s+hora)|(ten[eé]s\\s+la\\s+hora)|(tienes\\s+la\\s+hora)|(^|\\s)hora(\\s|$)'\n",
    "        t.value = 'que_hora_es'\n",
    "        return t\n",
    "\n",
    "    def t_HOLA(self, t):\n",
    "        r'(hola)|(buenas(\\s+(tardes|noches))?)|(buenos\\s+d[ií]as)'\n",
    "        t.value = 'hola'\n",
    "        return t\n",
    "\n",
    "    def t_QUIERO(self, t):\n",
    "        r'(quiero)|(quisiera)|(me\\s+gustar[ií]a)'\n",
    "        t.value = 'quiero'\n",
    "        return t\n",
    "\n",
    "    def t_INFORMACION(self, t):\n",
    "        r'(informaci[oó]n)|(info)|(data)|(detalles)'\n",
    "        t.value = 'informacion'\n",
    "        return t\n",
    "\n",
    "    def t_SOBRE(self, t):\n",
    "        r'(sobre)|(acerca\\s+de)|(de)'\n",
    "        t.value = 'sobre'\n",
    "        return t\n",
    "\n",
    "    def t_CURSOS(self, t):\n",
    "        r'(curso|cursos|capacitaci[oó]n|capacitaciones|formaci[oó]n)'\n",
    "        t.value = 'cursos'\n",
    "        return t\n",
    "\n",
    "    def t_TOPIC(self, t):\n",
    "        r'[A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9][A-Za-zÁÉÍÓÚÜÑáéíóúüñ0-9\\ \\-\\_]*'\n",
    "        t.value = t.value.strip()\n",
    "        return t\n",
    "\n",
    "    def t_error(self, t):\n",
    "        t.lexer.skip(1)\n",
    "\n",
    "# construir lexer y exponer _lex para helpers\n",
    "_lex = ChatLexer()\n",
    "lexer = lex.lex(module=_lex, reflags=re.IGNORECASE | re.UNICODE)\n",
    "\n",
    "#   PARSER (clase)  \n",
    "class ChatParser:\n",
    "    tokens = ChatLexer.tokens\n",
    "    precedence = ()\n",
    "\n",
    "    def __init__(self):\n",
    "        self.last_sem = None\n",
    "\n",
    "    def p_input(self, p):\n",
    "        '''input : input linea\n",
    "                 | linea'''\n",
    "        pass\n",
    "\n",
    "    def p_linea(self, p):\n",
    "        '''linea : saludo EOL\n",
    "                 | pregunta EOL\n",
    "                 | solicitud EOL\n",
    "                 | EOL'''\n",
    "        pass\n",
    "\n",
    "    def p_saludo(self, p):\n",
    "        'saludo : HOLA'\n",
    "        self.last_sem = {'intent': 'SALUDO'}\n",
    "\n",
    "    def p_pregunta(self, p):\n",
    "        'pregunta : QUEHORA'\n",
    "        self.last_sem = {'intent': 'PREGUNTA'}\n",
    "\n",
    "    # Variantes de solicitud:\n",
    "    # 1) QUIERO (INFORMACION)? (SOBRE)? CURSOS (SOBRE TOPIC)?\n",
    "    # 2) QUIERO CURSOS (SOBRE TOPIC)?\n",
    "    # 3) QUIERO INFORMACION (SOBRE TOPIC)?\n",
    "    def p_solicitud(self, p):\n",
    "        '''solicitud : QUIERO opt_info opt_sobre CURSOS opt_tema\n",
    "                     | QUIERO CURSOS opt_tema\n",
    "                     | QUIERO INFORMACION opt_tema'''\n",
    "        if len(p) == 6:\n",
    "            topic = p[5]\n",
    "        elif len(p) == 4:\n",
    "            topic = p[3]\n",
    "        else:\n",
    "            topic = None\n",
    "        self.last_sem = {'intent': 'SOLICITUD', 'topic': topic}\n",
    "\n",
    "    def p_opt_info(self, p):\n",
    "        '''opt_info : INFORMACION\n",
    "                    | empty'''\n",
    "        pass\n",
    "\n",
    "    def p_opt_sobre(self, p):\n",
    "        '''opt_sobre : SOBRE\n",
    "                     | empty'''\n",
    "        pass\n",
    "\n",
    "    def p_opt_tema(self, p):\n",
    "        '''opt_tema : empty\n",
    "                    | SOBRE tema'''\n",
    "        p[0] = p[2] if len(p) == 3 else None\n",
    "\n",
    "    def p_tema(self, p):\n",
    "        'tema : TOPIC'\n",
    "        p[0] = p[1]\n",
    "\n",
    "    def p_empty(self, p):\n",
    "        'empty :'\n",
    "        pass\n",
    "\n",
    "    def p_error(self, p):\n",
    "        # Consumir hasta EOL para seguir\n",
    "        if p is None: \n",
    "            return\n",
    "        tok = p\n",
    "        while tok and tok.type != 'EOL':\n",
    "            tok = parser.token()\n",
    "        self.last_sem = {'intent': 'ERROR'}\n",
    "\n",
    "_parser = ChatParser()\n",
    "parser  = yacc.yacc(module=_parser, start='input', write_tables=False, debug=False)\n",
    "\n",
    "#   Helpers pedidos  \n",
    "def tokenize(text: str):\n",
    "    \"\"\"Devuelve [(TYPE, value), ...] tokenizando con el lexer PLY del chatbot.\"\"\"\n",
    "    inst = lex.lex(module=_lex, reflags=re.IGNORECASE | re.UNICODE)  # lexer fresco\n",
    "    inst.input(text)\n",
    "    out = []\n",
    "    while True:\n",
    "        tok = inst.token()\n",
    "        if not tok:\n",
    "            break\n",
    "        out.append((tok.type, tok.value))\n",
    "    return out\n",
    "\n",
    "def parse_and_sem(text: str):\n",
    "    \"\"\"Parsea texto completo y devuelve la semántica (intent/topic).\"\"\"\n",
    "    parser.parse(text, lexer=lexer)\n",
    "    return _parser.last_sem\n",
    "\n",
    "# Pruebas\n",
    "print(\"PRUEBA:\")\n",
    "print(\"Tokens:\", tokenize(\"hola\\n\"))\n",
    "print(\"Sem   :\", parse_and_sem(\"hola\\n\"))\n",
    "print(\"Tokens:\", tokenize(\"que hora es\\n\"))\n",
    "print(\"Sem   :\", parse_and_sem(\"que hora es\\n\"))\n",
    "print(\"Tokens:\", tokenize(\"quiero informacion sobre cursos de deep learning\\n\"))\n",
    "print(\"Sem   :\", parse_and_sem(\"quiero informacion sobre cursos de deep learning\\n\"))\n",
    "\n",
    "# Respuestas por REGLAS\n",
    "# Usa el lexer PLY para obtener \"palabras\" compatibles con tus reglas.\n",
    "\n",
    "def _tokens_for_rules(text: str):\n",
    "    \"\"\"Convierte el texto en una lista de 'palabras' para el clasificador por reglas.\n",
    "       Aprovecha el lexer PLY ya definido arriba (_lex).\n",
    "    \"\"\"\n",
    "    inst = _lexmod.lex(module=_lex, reflags=re.IGNORECASE | re.UNICODE)\n",
    "    # Asegurar EOL para que el lexer/emparejador se comporte como en modo línea\n",
    "    if not text.endswith(\"\\n\"):\n",
    "        text = text + \"\\n\"\n",
    "    inst.input(text)\n",
    "\n",
    "    words = []\n",
    "    while True:\n",
    "        tok = inst.token()\n",
    "        if not tok:\n",
    "            break\n",
    "        # Expandimos tokens multi-palabra a palabras sueltas para tus reglas\n",
    "        if tok.type == \"QUEHORA\":\n",
    "            words += [\"que\", \"hora\", \"es\"]\n",
    "        elif tok.type in {\"HOLA\", \"QUIERO\", \"INFORMACION\", \"SOBRE\", \"CURSOS\", \"TOPIC\"}:\n",
    "            # 'value' puede venir como 'hola' o 'que_hora_es'; normalizamos split por '_' y espacios\n",
    "            words += tok.value.replace(\"_\", \" \").lower().split()\n",
    "        # Ignoramos EOL/puntuación/otros\n",
    "    return words\n",
    "\n",
    "def classify_intent(tokens):\n",
    "    # Gramática mínima implícita por patrones exactos\n",
    "    if tokens == ['hola'] or tokens == ['buenos', 'dias']:\n",
    "        return 'SALUDO'\n",
    "    if tokens == ['que', 'hora', 'es']:\n",
    "        return 'PREGUNTA_HORA'\n",
    "    if tokens == ['quiero', 'informacion', 'sobre', 'cursos']:\n",
    "        return 'SOLICITUD_INFO'\n",
    "    return 'DESCONOCIDO'\n",
    "\n",
    "def semantic_action(intent):\n",
    "    if intent == 'SALUDO':\n",
    "        return \"¡Hola! ¿Cómo estás?\"\n",
    "    if intent == 'PREGUNTA_HORA':\n",
    "        import datetime\n",
    "        return f\"La hora actual es {datetime.datetime.now().strftime('%H:%M:%S')}\"\n",
    "    if intent == 'SOLICITUD_INFO':\n",
    "        return \"Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\"\n",
    "    return \"No entendí tu consulta. ¿Podés reformularla?\"\n",
    "\n",
    "def chatbot_once(text: str):\n",
    "    \"\"\"Primero intenta con la CFG de PLY; si no clasifica, cae al clasificador por reglas.\"\"\"\n",
    "    # 1) Parser CFG\n",
    "    parser.parse(text if text.endswith(\"\\n\") else text + \"\\n\", lexer=lexer)\n",
    "    sem = _parser.last_sem\n",
    "    if sem and sem.get(\"intent\") in {\"SALUDO\", \"PREGUNTA\", \"SOLICITUD\"}:\n",
    "        if sem[\"intent\"] == \"SALUDO\":\n",
    "            return \"¡Hola! ¿Cómo estás?\"\n",
    "        if sem[\"intent\"] == \"PREGUNTA\":\n",
    "            import datetime\n",
    "            return f\"La hora actual es {datetime.datetime.now().strftime('%H:%M:%S')}\"\n",
    "        if sem[\"intent\"] == \"SOLICITUD\":\n",
    "            return \"Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\"\n",
    "\n",
    "    # 2) Fallback por reglas\n",
    "    toks = _tokens_for_rules(text)\n",
    "    intent = classify_intent(toks)\n",
    "    return semantic_action(intent)\n",
    "\n",
    "#   Pruebas rápidas integradas  \n",
    "print(\"\\nDEMOS (fallback por reglas si la CFG no matchea):\")\n",
    "for u in [\"hola\", \"que hora es\", \"quiero informacion sobre cursos\", \"adios\"]:\n",
    "    print(f\"Usuario: {u}\\nBot:     {chatbot_once(u)}\\n\")\n",
    "\n",
    "# SVO + Semántica\n",
    "# SVO = SUJETO+VERBO+OBJETO\n",
    "# Parser SVO por reglas simples (sin PLY) para testear errores semánticos.\n",
    "# Formato esperado (simplificado): ART SUST VERBO ART/AL SUST\n",
    "# Ej.: \"El perro muerde al perro\"  (válido: ANIMAL muerde ANIMAL)\n",
    "#     \"La idea muerde al perro\"   (inválido: ABSTRACTO no puede morder)\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode  # pip install unidecode\n",
    "\n",
    "#   Léxico / normalización  \n",
    "def tokenize_svo(text: str):\n",
    "    s = unidecode(text.strip().lower())\n",
    "    # normalizamos contracción 'al' -> 'el'\n",
    "    s = re.sub(r\"\\bal\\b\", \"el\", s)\n",
    "    # quitamos puntuación simple\n",
    "    s = re.sub(r\"[,\\.\\!\\?\\;\\:\\(\\)\\[\\]\\{\\}]+\", \" \", s)\n",
    "    toks = s.split()\n",
    "    return toks\n",
    "\n",
    "#   Modelo semántico mínimo  \n",
    "ENT = {}    # mapea (art, sust) -> id\n",
    "TYPES = {}  # mapea id -> tipo ('ANIMAL', 'ABSTRACTO', ...)\n",
    "\n",
    "# Diccionario de selección semántica para verbos\n",
    "# 'muerde': sujeto ANIMAL, objeto ANIMAL\n",
    "VERB_SIG = {\n",
    "    \"muerde\": {\"subj\": \"ANIMAL\", \"obj\": \"ANIMAL\"},\n",
    "    # Podés agregar más verbos acá\n",
    "}\n",
    "\n",
    "# Poblamos un par de entidades por defecto\n",
    "ENT[(\"el\", \"perro\")] = \"perro1\";   TYPES[\"perro1\"] = \"ANIMAL\"\n",
    "ENT[(\"el\", \"gato\")]  = \"gato1\";    TYPES[\"gato1\"]  = \"ANIMAL\"\n",
    "\n",
    "#   Parser por patrón (no PLY) + chequeo semántico  \n",
    "def parse_and_sem_svo(tokens):\n",
    "    \"\"\"\n",
    "    tokens: lista de strings (p. ej., ['la','idea','muerde','el','perro'])\n",
    "    Devuelve dict con semántica o lanza ValueError en caso de error semántico.\n",
    "    \"\"\"\n",
    "    if len(tokens) < 5:\n",
    "        raise ValueError(f\"Estructura incompleta: {tokens}\")\n",
    "\n",
    "    # Esperamos: ART SUST VERBO ART SUST\n",
    "    art1, sust1, verbo, art2, sust2 = tokens[:5]\n",
    "\n",
    "    # Resolver entidades\n",
    "    key1 = (art1, sust1)\n",
    "    key2 = (art2, sust2)\n",
    "\n",
    "    if key1 not in ENT:\n",
    "        raise ValueError(f\"Sujeto desconocido: {key1}\")\n",
    "    if key2 not in ENT:\n",
    "        raise ValueError(f\"Objeto desconocido: {key2}\")\n",
    "\n",
    "    subj_id = ENT[key1]; subj_ty = TYPES.get(subj_id, \"DESCONOCIDO\")\n",
    "    obj_id  = ENT[key2]; obj_ty  = TYPES.get(obj_id,  \"DESCONOCIDO\")\n",
    "\n",
    "    # Selección semántica del verbo\n",
    "    sig = VERB_SIG.get(verbo)\n",
    "    if not sig:\n",
    "        raise ValueError(f\"Verbo sin firma semántica definida: '{verbo}'\")\n",
    "\n",
    "    need_subj = sig.get(\"subj\")\n",
    "    need_obj  = sig.get(\"obj\")\n",
    "\n",
    "    if need_subj and subj_ty != need_subj:\n",
    "        raise ValueError(f\"Tipo de SUJETO inválido para '{verbo}': se requiere {need_subj}, llegó {subj_ty}\")\n",
    "    if need_obj and obj_ty != need_obj:\n",
    "        raise ValueError(f\"Tipo de OBJETO inválido para '{verbo}': se requiere {need_obj}, llegó {obj_ty}\")\n",
    "\n",
    "    # Si todo OK, devolvemos una estructura semántica\n",
    "    return {\n",
    "        \"pred\": verbo,\n",
    "        \"subj\": {\"id\": subj_id, \"type\": subj_ty, \"form\": f\"{art1} {sust1}\"},\n",
    "        \"obj\":  {\"id\": obj_id,  \"type\": obj_ty,  \"form\": f\"{art2} {sust2}\"},\n",
    "    }\n",
    "\n",
    "   \n",
    "# Agregamos una entidad no animal y su tipo ABSTRACTO\n",
    "ENT[(\"la\", \"idea\")] = \"idea1\"\n",
    "TYPES[\"idea1\"] = \"ABSTRACTO\"\n",
    "\n",
    "# Caso que debe FALLAR semánticamente:\n",
    "try:\n",
    "    bad_tokens = tokenize_svo(\"La idea muerde al perro\")\n",
    "    print(\"Tokens (bad):\", bad_tokens)\n",
    "    print(parse_and_sem_svo(bad_tokens))\n",
    "except Exception as e:\n",
    "    print(\"Error semántico detectado:\", e)\n",
    "\n",
    "# Caso que debe PASAR:\n",
    "try:\n",
    "    ok_tokens = tokenize_svo(\"El perro muerde al perro\")\n",
    "    print(\"Tokens (ok):\", ok_tokens)\n",
    "    print(parse_and_sem_svo(ok_tokens))\n",
    "except Exception as e:\n",
    "    print(\"Error inesperado:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd3b4",
   "metadata": {},
   "source": [
    "## 5. Parser con NLTK y construcción del árbol sintáctico\n",
    "- Ejecutar el siguiente bloque para construir un `ChartParser` con una gramática mínima y obtener árboles de derivación.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84f0c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Árboles de parse para: ['el', 'gato', 'muerde', 'a', 'el', 'perro']\n",
      "(S\n",
      "  (NP (Det el) (N gato))\n",
      "  (VP (V muerde) (NP_a a (NP (Det el) (N perro)))))\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar este bloque sólo si NLTK disponible en entorno.\n",
    "try:\n",
    "    import nltk\n",
    "    grammar   nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | N\n",
    "    VP -> V NP_a\n",
    "    NP_a -> 'a' NP\n",
    "    Det -> 'el'\n",
    "    N -> 'gato' | 'perro'\n",
    "    V -> 'muerde'\n",
    "    \"\"\")\n",
    "    parser   nltk.ChartParser(grammar)\n",
    "    sent   ['el','gato','muerde','a','el','perro']\n",
    "    print(\"Árboles de parse para:\", sent)\n",
    "    for tree in parser.parse(sent):\n",
    "        print(tree)\n",
    "except ModuleNotFoundError as e:\n",
    "    print(\"NLTK no está instalado en este entorno. Instalarlo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b832d1b-5fdb-4ed0-bef9-e89718138722",
   "metadata": {},
   "source": [
    "## 6. Clasificador de intents con una red neuronal simple usando scikit-learn \n",
    "\n",
    "### NOTA: ESTE TEMA ESTA FUERA DEL ALCANCE DE LA MATERIA. SE PRESENTA EL EJEMPLO A MODO DE PRUEBA.\n",
    "\n",
    "#### Ejemplo 1 \n",
    "\n",
    "- Arma un dataset ejemplo (SALUDO / PREGUNTA_HORA / SOLICITUD), normaliza el texto (minúsculas + sin acentos), vectoriza con TF-IDF n-grams y entrena un MLP (una capa oculta).\n",
    "- Al final evalúa y prueba con frases nuevas.\n",
    "- Normalización: lowercase + quitar acentos.\n",
    "- Vectorización: TF-IDF de 1-2 grams (captura “que hora”).\n",
    "- Red: MLPClassifier con una capa oculta de 64 neuronas (ReLU + Adam).\n",
    "- Clases: SALUDO, PREGUNTA_HORA, SOLICITUD.\n",
    "\n",
    "#### Ejemplo 2\n",
    "- Detección/corrección léxica: compara cada token contra el vocabulario de entrenamiento y propone la mejor coincidencia ( umbral cutoff 0.83).\n",
    "- Ej.: “informaxion” → “informacion”.\n",
    "- Si no encuentra palabra, lo marca como desconocido pero igual intenta clasificar.\n",
    "- Diagnóstico en consola: muestra texto normalizado, tokens corregidos, OOV, intent + confianza y top-3.\n",
    "- UI (ipywidgets): campo de texto + botón. Imprime diagnóstico y respuesta.\n",
    "- Respuestas: SALUDO, PREGUNTA_HORA (con hora del sistema), SOLICITUD; si no entiende, devuelve una respuesta amable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b34845-e9dd-4158-9f2a-1259f51ee89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Reporte en test ==\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "PREGUNTA_HORA      1.000     1.000     1.000         2\n",
      "       SALUDO      1.000     1.000     1.000         3\n",
      "    SOLICITUD      1.000     1.000     1.000         2\n",
      "\n",
      "     accuracy                          1.000         7\n",
      "    macro avg      1.000     1.000     1.000         7\n",
      " weighted avg      1.000     1.000     1.000         7\n",
      "\n",
      "\n",
      "== Predicciones ==\n",
      "- 'Hola!' -> SALUDO  [SALUDO:0.97, SOLICITUD:0.02, PREGUNTA_HORA:0.01]\n",
      "- 'Me decís la hora por favor?' -> PREGUNTA_HORA  [PREGUNTA_HORA:1.00, SOLICITUD:0.00, SALUDO:0.00]\n",
      "- 'Quisiera información sobre cursos de deep learning' -> SOLICITUD  [SOLICITUD:1.00, SALUDO:0.00, PREGUNTA_HORA:0.00]\n",
      "- 'Buenas noches' -> SALUDO  [SALUDO:0.99, PREGUNTA_HORA:0.00, SOLICITUD:0.00]\n",
      "- 'La hora ahora' -> PREGUNTA_HORA  [PREGUNTA_HORA:0.98, SALUDO:0.01, SOLICITUD:0.01]\n",
      "- 'Necesito capacitaciones en simulacion' -> SOLICITUD  [SOLICITUD:0.79, SALUDO:0.14, PREGUNTA_HORA:0.07]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Red neuronal simple (MLP) para intents: SALUDO, PREGUNTA_HORA, SOLICITUD\n",
    "\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "  \n",
    "# 1) Dataset mini (ejemplo)\n",
    "  \n",
    "saludos = [\n",
    "    \"hola\", \"buenas\", \"buenos dias\", \"buenas tardes\", \"buenas noches\",\n",
    "    \"hola como estas\", \"hola que tal\", \"hola, buen dia\"\n",
    "]\n",
    "preg_hora = [\n",
    "    \"que hora es\", \"me decis la hora\", \"tenes la hora\", \"tienes la hora\",\n",
    "    \"me podrias decir la hora\", \"que hora es ahora\", \"la hora por favor\"\n",
    "]\n",
    "solicitudes = [\n",
    "    \"quiero informacion sobre cursos\", \"quisiera cursos de redes\",\n",
    "    \"me gustaria informacion de cursos\", \"quiero cursos sobre machine learning\",\n",
    "    \"necesito informacion sobre capacitaciones\", \"informacion de formacion por favor\",\n",
    "    \"informacion sobre cursos de simulacion\", \"quiero curso de deep learning\"\n",
    "]\n",
    "\n",
    "X = saludos + preg_hora + solicitudes\n",
    "y = ([\"SALUDO\"] * len(saludos)\n",
    "     + [\"PREGUNTA_HORA\"] * len(preg_hora)\n",
    "     + [\"SOLICITUD\"] * len(solicitudes))\n",
    "\n",
    "  \n",
    "# 2) Normalización simple\n",
    "  \n",
    "def normalize(s: str) -> str:\n",
    "    # minúsculas + sin acentos + colapsar espacios\n",
    "    s = unidecode(s.lower())\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "X_norm = [normalize(t) for t in X]\n",
    "\n",
    "  \n",
    "# 3) Split train/test\n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "  \n",
    "# 4) Pipeline TF-IDF + MLP\n",
    "  \n",
    "# n-grams (1 a 2) suelen ayudar en frases cortas\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
    "    (\"mlp\", MLPClassifier(\n",
    "        hidden_layer_sizes=(64,),  # una capa oculta de 64 neuronas\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        batch_size=8,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "  \n",
    "# 5) Evaluación\n",
    "  \n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"== Reporte en test ==\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "  \n",
    "# 6) Pruebas manuales\n",
    "  \n",
    "pruebas = [\n",
    "    \"Hola!\", \n",
    "    \"Me decís la hora por favor?\",\n",
    "    \"Quisiera información sobre cursos de deep learning\",\n",
    "    \"Buenas noches\",\n",
    "    \"La hora ahora\",\n",
    "    \"Necesito capacitaciones en simulacion\",\n",
    "]\n",
    "pruebas_norm = [normalize(p) for p in pruebas]\n",
    "preds = pipe.predict(pruebas_norm)\n",
    "proba = pipe.predict_proba(pruebas_norm)\n",
    "labels = pipe.classes_\n",
    "\n",
    "print(\"\\n== Predicciones ==\")\n",
    "for txt, pred, pr in zip(pruebas, preds, proba):\n",
    "    top = sorted(list(zip(labels, pr)), key=lambda x: x[1], reverse=True)[:3]\n",
    "    conf = \", \".join([f\"{lbl}:{p:.2f}\" for lbl, p in top])\n",
    "    print(f\"- '{txt}' -> {pred}  [{conf}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847568b-826a-4591-a920-f58138c1f31d",
   "metadata": {},
   "source": [
    "#### Segundo ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db44a471-c278-4ae6-b62d-7044b6af3a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Reporte en test ==\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "PREGUNTA_HORA      1.000     1.000     1.000         2\n",
      "       SALUDO      1.000     1.000     1.000         3\n",
      "    SOLICITUD      1.000     1.000     1.000         2\n",
      "\n",
      "     accuracy                          1.000         7\n",
      "    macro avg      1.000     1.000     1.000         7\n",
      " weighted avg      1.000     1.000     1.000         7\n",
      "\n",
      "\n",
      "== Predicciones y respuestas (con detección/corrección lexical) ==\n",
      "- Usuario: Hola!\n",
      "DETECCION ERRORES:\n",
      "original   : Hola!\n",
      "normalized : hola!\n",
      "corrected  : hola\n",
      "lex-issues : ['hola!→hola(corregido)']\n",
      "intent     : SALUDO (conf=0.97)\n",
      "top3       : SALUDO:0.97, SOLICITUD:0.02, PREGUNTA_HORA:0.01\n",
      "    ==\n",
      "  Bot   : ¡Hola! ¿Cómo estás?\n",
      "\n",
      "- Usuario: Me decis la hora porfas?\n",
      "DETECCION ERRORES:\n",
      "original   : Me decis la hora porfas?\n",
      "normalized : me decis la hora porfas?\n",
      "corrected  : me decis la hora porfas?\n",
      "lex-issues : ['porfas?→∅(desconocido)']\n",
      "intent     : PREGUNTA_HORA (conf=0.99)\n",
      "top3       : PREGUNTA_HORA:0.99, SALUDO:0.00, SOLICITUD:0.00\n",
      "    ==\n",
      "  Bot   : Son las 09:48:45.\n",
      "\n",
      "- Usuario: Quisiera informaxion sobre cursos\n",
      "DETECCION ERRORES:\n",
      "original   : Quisiera informaxion sobre cursos\n",
      "normalized : quisiera informaxion sobre cursos\n",
      "corrected  : quisiera informacion sobre cursos\n",
      "lex-issues : ['informaxion→informacion(corregido)']\n",
      "intent     : SOLICITUD (conf=0.99)\n",
      "top3       : SOLICITUD:0.99, SALUDO:0.00, PREGUNTA_HORA:0.00\n",
      "    ==\n",
      "  Bot   : Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\n",
      "\n",
      "- Usuario: Bunas nochess\n",
      "DETECCION ERRORES:\n",
      "original   : Bunas nochess\n",
      "normalized : bunas nochess\n",
      "corrected  : buenas noches\n",
      "lex-issues : ['bunas→buenas(corregido)', 'nochess→noches(corregido)']\n",
      "intent     : SALUDO (conf=0.99)\n",
      "top3       : SALUDO:0.99, PREGUNTA_HORA:0.00, SOLICITUD:0.00\n",
      "    ==\n",
      "  Bot   : ¡Hola! ¿Cómo estás?\n",
      "\n",
      "- Usuario: Kiero cursoz sobr redes\n",
      "DETECCION ERRORES:\n",
      "original   : Kiero cursoz sobr redes\n",
      "normalized : kiero cursoz sobr redes\n",
      "corrected  : kiero curso sobre redes\n",
      "lex-issues : ['kiero→∅(desconocido)', 'cursoz→curso(corregido)', 'sobr→sobre(corregido)']\n",
      "intent     : SOLICITUD (conf=0.90)\n",
      "top3       : SOLICITUD:0.90, SALUDO:0.05, PREGUNTA_HORA:0.04\n",
      "    ==\n",
      "  Bot   : Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\n",
      "\n",
      "- Usuario: La hora ahora\n",
      "DETECCION ERRORES:\n",
      "original   : La hora ahora\n",
      "normalized : la hora ahora\n",
      "corrected  : la hora ahora\n",
      "intent     : PREGUNTA_HORA (conf=0.98)\n",
      "top3       : PREGUNTA_HORA:0.98, SALUDO:0.01, SOLICITUD:0.01\n",
      "    ==\n",
      "  Bot   : Son las 09:48:45.\n",
      "\n",
      "- Usuario: Necesito capacitaciones en simulacion\n",
      "DETECCION ERRORES:\n",
      "original   : Necesito capacitaciones en simulacion\n",
      "normalized : necesito capacitaciones en simulacion\n",
      "corrected  : necesito capacitaciones en simulacion\n",
      "lex-issues : ['en→∅(desconocido)']\n",
      "intent     : SOLICITUD (conf=0.79)\n",
      "top3       : SOLICITUD:0.79, SALUDO:0.14, PREGUNTA_HORA:0.07\n",
      "    ==\n",
      "  Bot   : Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Chat demo — MLP + Corrección léxica</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519aebc554ee414cb68142accbdc1fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='Hlaaa bnas nochess, me decis la hroa?', description='Tu mensaje:', layout=Layout(wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Clasificador de intents con MLP + manejo de \"errores léxicos\" (corrección simple)\n",
    "# Requiere: scikit-learn, unidecode. (UI opcional: ipywidgets, pandas)\n",
    "\n",
    "from unidecode import unidecode\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import difflib\n",
    "import datetime\n",
    "\n",
    "#   1) Dataset mini   \n",
    "saludos = [\n",
    "    \"hola\", \"buenas\", \"buenos dias\", \"buenas tardes\", \"buenas noches\",\n",
    "    \"hola como estas\", \"hola que tal\", \"hola, buen dia\"\n",
    "]\n",
    "preg_hora = [\n",
    "    \"que hora es\", \"me decis la hora\", \"tenes la hora\", \"tienes la hora\",\n",
    "    \"me podrias decir la hora\", \"que hora es ahora\", \"la hora por favor\"\n",
    "]\n",
    "solicitudes = [\n",
    "    \"quiero informacion sobre cursos\", \"quisiera cursos de redes\",\n",
    "    \"me gustaria informacion de cursos\", \"quiero cursos sobre machine learning\",\n",
    "    \"necesito informacion sobre capacitaciones\", \"informacion de formacion por favor\",\n",
    "    \"informacion sobre cursos de simulacion\", \"quiero curso de deep learning\"\n",
    "]\n",
    "\n",
    "X = saludos + preg_hora + solicitudes\n",
    "y = ([\"SALUDO\"] * len(saludos)\n",
    "     + [\"PREGUNTA_HORA\"] * len(preg_hora)\n",
    "     + [\"SOLICITUD\"] * len(solicitudes))\n",
    "\n",
    "#   2) Normalización & tokenización   \n",
    "def normalize(s: str) -> str:\n",
    "    s = unidecode(s.lower())\n",
    "    s = \" \".join(s.split())\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return normalize(s).split()\n",
    "\n",
    "X_norm = [normalize(t) for t in X]\n",
    "\n",
    "#   3) Split   \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_norm, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "#  4) Pipeline TF-IDF + MLP   \n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
    "    (\"mlp\", MLPClassifier(\n",
    "        hidden_layer_sizes=(64,),\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        alpha=1e-3,\n",
    "        batch_size=8,\n",
    "        learning_rate_init=1e-3,\n",
    "        max_iter=300,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "#  5) Evaluación rápida   \n",
    "y_pred = pipe.predict(X_test)\n",
    "print(\"== Reporte en test ==\")\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "#  6) Vocabulario para detectar/corregir OOV   \n",
    "# Construimos un vocabulario \"léxico\" básico desde el set de tokens de entrenamiento\n",
    "lex_vocab = set()\n",
    "for s in X_norm:\n",
    "    lex_vocab.update(s.split())\n",
    "\n",
    "def detect_and_correct_tokens(text: str, vocab: set[str], cutoff: float = 0.83):\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      corrected_tokens: lista de tokens (posiblemente corregidos)\n",
    "      oov_report: lista de dicts con token original, sugerencia y confianza aproximada\n",
    "    \"\"\"\n",
    "    toks = tokenize(text)\n",
    "    corrected = []\n",
    "    report = []\n",
    "    for t in toks:\n",
    "        if t in vocab:\n",
    "            corrected.append(t)\n",
    "        else:\n",
    "            # Proponemos la mejor coincidencia del vocabulario (difflib)\n",
    "            cand = difflib.get_close_matches(t, list(vocab), n=1, cutoff=cutoff)\n",
    "            if cand:\n",
    "                corrected.append(cand[0])\n",
    "                report.append({\"token\": t, \"suggestion\": cand[0], \"note\": \"corregido\"})\n",
    "            else:\n",
    "                corrected.append(t)  # sin corrección\n",
    "                report.append({\"token\": t, \"suggestion\": None, \"note\": \"desconocido\"})\n",
    "    return corrected, report\n",
    "\n",
    "#  7) Predicción + respuesta (aunque haya errores léxicos)   \n",
    "def predict_intent_with_lex(text: str):\n",
    "    \"\"\"\n",
    "    - Normaliza, detecta OOV y corrige.\n",
    "    - Predice con el texto corregido.\n",
    "    - Devuelve intent, probas y reporte de OOV.\n",
    "    \"\"\"\n",
    "    corrected_tokens, oov = detect_and_correct_tokens(text, lex_vocab)\n",
    "    corrected_text = \" \".join(corrected_tokens)\n",
    "    proba = pipe.predict_proba([corrected_text])[0]\n",
    "    labels = pipe.classes_\n",
    "    top_idx = int(np.argmax(proba))\n",
    "    intent = labels[top_idx]\n",
    "    conf = float(proba[top_idx])\n",
    "    top3 = sorted(list(zip(labels, proba)), key=lambda x: x[1], reverse=True)[:3]\n",
    "    return {\n",
    "        \"original_text\": text,\n",
    "        \"normalized\": normalize(text),\n",
    "        \"corrected_text\": corrected_text,\n",
    "        \"oov\": oov,\n",
    "        \"intent\": intent,\n",
    "        \"confidence\": conf,\n",
    "        \"top3\": top3\n",
    "    }\n",
    "\n",
    "def generate_reply(intent: str) -> str:\n",
    "    if intent == \"SALUDO\":\n",
    "        return \"¡Hola! ¿Cómo estás?\"\n",
    "    if intent == \"PREGUNTA_HORA\":\n",
    "        # Hora local del sistema\n",
    "        return f\"Son las {datetime.datetime.now().strftime('%H:%M:%S')}.\"\n",
    "    if intent == \"SOLICITUD\":\n",
    "        return \"Tenemos cursos de Machine Learning, Simulación y Redes. ¿Cuál te interesa?\"\n",
    "    # fallback\n",
    "    return \"No entendí bien tu consulta, pero puedo ayudarte con saludos, la hora o información de cursos.\"\n",
    "\n",
    "def chat_once(text: str, show_debug: bool = True):\n",
    "    info = predict_intent_with_lex(text)\n",
    "    reply = generate_reply(info[\"intent\"])\n",
    "    if show_debug:\n",
    "        print(\"DETECCION ERRORES:\")\n",
    "        print(\"original   :\", info[\"original_text\"])\n",
    "        print(\"normalized :\", info[\"normalized\"])\n",
    "        print(\"corrected  :\", info[\"corrected_text\"])\n",
    "        if info[\"oov\"]:\n",
    "            print(\"lex-issues :\",\n",
    "                  [f\"{r['token']}→{r['suggestion'] or '∅'}({r['note']})\" for r in info[\"oov\"]])\n",
    "        print(\"intent     :\", info[\"intent\"], f\"(conf={info['confidence']:.2f})\")\n",
    "        print(\"top3       :\", \", \".join([f\"{l}:{p:.2f}\" for l,p in info[\"top3\"]]))\n",
    "        print(\"    ==\")\n",
    "    return reply\n",
    "\n",
    "#  8) Pruebas manuales (incluye errores léxicos)   \n",
    "tests = [\n",
    "    \"Hola!\",\n",
    "    \"Me decis la hora porfas?\",         # 'porfas' no está; se intentará corregir\n",
    "    \"Quisiera informaxion sobre cursos\",# 'informaxion' error\n",
    "    \"Bunas nochess\",                    # errores múltiples\n",
    "    \"Kiero cursoz sobr redes\",          # leetspeak-ish\n",
    "    \"La hora ahora\",\n",
    "    \"Necesito capacitaciones en simulacion\",\n",
    "]\n",
    "print(\"\\n== Predicciones y respuestas (con detección/corrección lexical) ==\")\n",
    "for t in tests:\n",
    "    print(f\"- Usuario: {t}\")\n",
    "    print(f\"  Bot   : {chat_once(t, show_debug=True)}\\n\")\n",
    "\n",
    "#  9) UI interactiva con ipywidgets (si está disponible)   \n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, clear_output, HTML\n",
    "\n",
    "    inp = widgets.Text(\n",
    "        value=\"Hlaaa bnas nochess, me decis la hroa?\",\n",
    "        description=\"Tu mensaje:\",\n",
    "        layout=widgets.Layout(width=\"100%\")\n",
    "    )\n",
    "    btn = widgets.Button(description=\"Enviar\", button_style=\"primary\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    def on_click(_):\n",
    "        with out:\n",
    "            clear_output()\n",
    "            resp = chat_once(inp.value, show_debug=True)\n",
    "            print(\"\\nRespuesta:\", resp)\n",
    "\n",
    "    btn.on_click(on_click)\n",
    "    box = widgets.VBox([inp, btn, out])\n",
    "    display(HTML(\"<h3>Chat — MLP + Corrección léxica</h3>\"))\n",
    "    display(box)\n",
    "except Exception:\n",
    "    print(\"\\n[UI] ipywidgets no disponible. Usá chat_once('...') en celdas o hacé un loop input().\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1eb69f",
   "metadata": {},
   "source": [
    "## 8. Ejercicios propuestos\n",
    "- Extender el chatbot con m{as frases y tokens.\n",
    "- Hacer el chatbot interactivo desde la terminal.\n",
    "- Analizar errores semánticos del chatbot\n",
    "- Extender el léxico agregando `('el','raton') → raton1` y probando frases:\n",
    "   - \"el perro muerde a el raton\"\n",
    "   - \"el raton muerde a el perro\"\n",
    "- Agregar un segundo verbo `persigue` con semántica `PERSIGUE(subj, obj)` y mostrar:\n",
    "   - `PERSIGUE(perro1, gato1)`\n",
    "- Control de tipos: forzar un error semántico con un sujeto no animal e interpretar el mensaje.\n",
    "- Chatbot: agregar una intención `AYUDA` para frases como *\"ayuda\"*, *\"necesito ayuda\"* y responder con un menú.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b6c33",
   "metadata": {},
   "source": [
    "## 9. Referencias y notas\n",
    "- Aho, Lam, Sethi, Ullman. **Compilers: Principles, Techniques, and Tools**.\n",
    "- Jurafsky & Martin. **Speech and Language Processing**.\n",
    "- Hopcroft, Motwani, Ullman. **Introduction to Automata Theory, Languages, and Computation**.\n",
    "- Notas de clase: **CFG, SDT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ddb70a-4e2f-4619-bf7b-9cf11ed81eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venvmys)",
   "language": "python",
   "name": "venvmys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
